{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d756c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import truncnorm\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92074f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, config, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = 1 \n",
    "        current_h, current_w = 28, 28 \n",
    "\n",
    "        for i in range(config['num_conv_layers']):\n",
    "            out_channels = config['filters'][i]\n",
    "            kernel_size = config['filter_sizes'][i]\n",
    "            padding = kernel_size // 2\n",
    "\t\t\t\t\t\t# prevent kernel size > feature size \n",
    "            actual_kernel_size = min(kernel_size, current_h, current_w)\n",
    "            actual_padding = actual_kernel_size // 2\n",
    "\n",
    "            conv = nn.Conv2d(in_channels, out_channels, actual_kernel_size, padding=actual_padding)\n",
    "            self.layers.append(conv)\n",
    "            if config['conv_activations'][i] == 1:  # relu\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif config['conv_activations'][i] == 2:  # tanh\n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif config['conv_activations'][i] == 3:  # linear\n",
    "                self.layers.append(nn.Identity())\n",
    "            elif config['conv_activations'][i] == 4:  # sigmoid\n",
    "                self.layers.append(nn.Sigmoid())\n",
    "\n",
    "            if config['pooling_types'][i] != 0: # 0 no, 1 max, 2 avg \n",
    "                pool_size = config['pool_sizes'][i]\n",
    "                actual_pool_size = min(pool_size, current_h, current_w)\n",
    "                \n",
    "                if actual_pool_size >= 2 :\n",
    "                    if config['pooling_types'][i] == 1:\n",
    "                        self.layers.append(nn.MaxPool2d(actual_pool_size))\n",
    "                    elif config['pooling_types'][i] == 2:\n",
    "                         self.layers.append(nn.AvgPool2d(actual_pool_size))\n",
    "                    \n",
    "                    current_h //= actual_pool_size\n",
    "                    current_w //= actual_pool_size\n",
    "                    if current_h == 0 or current_w == 0:\n",
    "                        raise ValueError(\"Invalid hehehe pooling\")\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.layers.append(nn.Flatten())\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 28, 28) \n",
    "            temp_layers_for_flatten_calc = nn.Sequential(*[l for l in self.layers if not isinstance(l, nn.Flatten)])\n",
    "            x_conv_out = temp_layers_for_flatten_calc(dummy_input)\n",
    "            flatten_layer_temp = nn.Flatten()\n",
    "            flattened_output_temp = flatten_layer_temp(x_conv_out)\n",
    "            flattened_size = flattened_output_temp.shape[1]\n",
    "\n",
    "            if flattened_size == 0:\n",
    "                 raise ValueError(\"Flattened size is 0\")\n",
    "\n",
    "        in_features_dense = flattened_size\n",
    "        for i in range(config['num_dense_layers']):\n",
    "            out_features = config['dense_units'][i]\n",
    "            dense = nn.Linear(in_features_dense, out_features)\n",
    "            self.layers.append(dense)\n",
    "\n",
    "            if i < config['num_dense_layers'] : # apply activation to all hidden dense layers except output layer \n",
    "                if config['dense_activations'][i] == 1:\n",
    "                    self.layers.append(nn.ReLU())\n",
    "                elif config['dense_activations'][i] == 2:\n",
    "                    self.layers.append(nn.Tanh())\n",
    "                elif config['dense_activations'][i] == 3:\n",
    "                    self.layers.append(nn.Identity())\n",
    "                elif config['dense_activations'][i] == 4:\n",
    "                    self.layers.append(nn.Sigmoid())\n",
    "            in_features_dense = out_features\n",
    "\n",
    "        self.output_layer_final = nn.Linear(in_features_dense, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer_final(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa58c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSA:\n",
    "    def __init__(self, hms=10, hmcr=0.85, par=0.3, bw_factor=0.1, max_improvs=50, dataset_name='digits'):\n",
    "        self.hms = hms  # Harmony Memory Size\n",
    "        self.hmcr = hmcr  # Harmony Memory Considering Rate\n",
    "        self.par = par  # Pitch Adjusting Rate\n",
    "        self.bw_factor = bw_factor # Bandwidth factor for pitch adjustment of continuous-like params\n",
    "        self.max_improvs = max_improvs\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        self.train_dataset = torchvision.datasets.EMNIST(root='./data', split=dataset_name,train=True, transform=transform, download=True)\n",
    "        self.test_dataset = torchvision.datasets.EMNIST(root='./data', split=dataset_name,train=False, transform=transform)\n",
    "        self.num_classes = len(self.train_dataset.classes)\n",
    "\n",
    "\n",
    "        self.hyperparameter_ranges = {\n",
    "            'num_epochs': (1, 49), \n",
    "            'batch_size': [32, 64, 128, 256],\n",
    "            'num_conv_layers': (1, 9), \n",
    "            'filters': (1, 65), \n",
    "            'filter_sizes': [3, 5],  \n",
    "            'conv_activations': (1, 4),  # 1: ReLU, 2: Tanh, 3: Linear, 4: Sigmoid\n",
    "            'pooling_types': (0, 2),  #0 none, 1 max, 2 avg\n",
    "            'pool_sizes': [2, 3], # Pooling kernel sizes\n",
    "            'num_dense_layers': (1,9),\n",
    "            'dense_units': (1, 65), # Range for units in dense layers\n",
    "            'dense_activations': (1, 4),\n",
    "            'optimizer': (1, 4),  # 1: SGD, 2: Adam, 3: RMSprop, 4: Adadelta\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1] # Discrete choices\n",
    "        }\n",
    "        self.harmony_memory = [] \n",
    "    def _generate_random_value(self, key):\n",
    "        if key == 'batch_size' or key == 'filter_sizes' or key == 'pool_sizes' or key == 'learning_rate':\n",
    "            return random.choice(self.hyperparameter_ranges[key])\n",
    "        elif isinstance(self.hyperparameter_ranges[key], tuple) and len(self.hyperparameter_ranges[key]) == 2:\n",
    "             min_val, max_val = self.hyperparameter_ranges[key]\n",
    "             if isinstance(min_val, int):\n",
    "                return np.random.randint(min_val, max_val + 1) #\n",
    "             else: \n",
    "                return np.random.uniform(min_val, max_val)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown type for hyperparameter range: {key}\")\n",
    "\n",
    "    def initialize_harmony(self):\n",
    "        config = {}\n",
    "        config['num_epochs'] = self._generate_random_value('num_epochs')\n",
    "        config['batch_size'] = self._generate_random_value('batch_size')\n",
    "        config['num_conv_layers'] = self._generate_random_value('num_conv_layers')\n",
    "        \n",
    "        config['filters'] = [self._generate_random_value('filters') for _ in range(config['num_conv_layers'])]\n",
    "        config['filter_sizes'] = [self._generate_random_value('filter_sizes') for _ in range(config['num_conv_layers'])]\n",
    "        config['conv_activations'] = [self._generate_random_value('conv_activations') for _ in range(config['num_conv_layers'])]\n",
    "        config['pooling_types'] = [self._generate_random_value('pooling_types') for _ in range(config['num_conv_layers'])]\n",
    "        config['pool_sizes'] = [self._generate_random_value('pool_sizes') for _ in range(config['num_conv_layers'])]\n",
    "        \n",
    "        config['num_dense_layers'] = self._generate_random_value('num_dense_layers')\n",
    "        config['dense_units'] = [self._generate_random_value('dense_units') for _ in range(config['num_dense_layers'])]\n",
    "        config['dense_activations'] = [self._generate_random_value('dense_activations') for _ in range(config['num_dense_layers'])]\n",
    "        \n",
    "        config['optimizer'] = self._generate_random_value('optimizer')\n",
    "        config['learning_rate'] = self._generate_random_value('learning_rate')\n",
    "        return config\n",
    "\n",
    "    def evaluate_harmony(self, config):\n",
    "        try:\n",
    "            model = CNN(config, self.num_classes).to(device)\n",
    "            \n",
    "            lr = config['learning_rate']\n",
    "            if config['optimizer'] == 1:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9) \n",
    "            elif config['optimizer'] == 2:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            elif config['optimizer'] == 3:\n",
    "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "            elif config['optimizer'] == 4:\n",
    "                 optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "            else: # Default to Adam\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            train_loader = DataLoader(self.train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(self.test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "            best_val_accuracy_for_this_config = 0.0\n",
    "            # early stopping \n",
    "            # tolerance  = 3\n",
    "            # tol_count = 0\n",
    "\n",
    "            for epoch in range(config['num_epochs']):\n",
    "                model.train()\n",
    "                for images, labels in train_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in test_loader: \n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_val_accuracy = correct / total\n",
    "                if current_val_accuracy > best_val_accuracy_for_this_config:\n",
    "                    best_val_accuracy_for_this_config = current_val_accuracy\n",
    "                    # tol_count = 0 # reset \n",
    "                # else:\n",
    "                    # tol_count += 1\n",
    "                \n",
    "                # if tol_count >= tolerance:\n",
    "                #     print(f\"early stopping at epoch {epoch+1} for this config.\")\n",
    "                #     break \n",
    "                # print(f\"Epoch {epoch+1}/{config['num_epochs']}, Accuracy: {current_val_accuracy:.4f} (Best this config: {best_val_accuracy_for_this_config:.4f})\")\n",
    "\n",
    "\n",
    "            return best_val_accuracy_for_this_config \n",
    "\n",
    "        except (ValueError, RuntimeError) as e:\n",
    "            print(f\"Evaluation failed for config. Error: {e}\")\n",
    "            # print(f\"INvalid Config: {config}\") # debugging \n",
    "            return 0.0 # invalid architectures ko 0 fitness\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during evaluation: {e}\")\n",
    "            # print(f\"INvalid Config: {config}\") #debugging\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "    def _pitch_adjust_value(self, current_value, key, parent_config_for_lengths=None):\n",
    "        \"\"\"Adjusts a single value for a hyperparameter key.\"\"\"\n",
    "        param_spec = self.hyperparameter_ranges[key]\n",
    "\n",
    "        if key in ['batch_size', 'filter_sizes', 'pool_sizes', 'learning_rate']:\n",
    "            choices = list(param_spec)\n",
    "            if current_value in choices: choices.remove(current_value)\n",
    "            return random.choice(choices) if choices else current_value\n",
    "        \n",
    "        elif isinstance(param_spec, tuple) and len(param_spec) == 2:\n",
    "            min_val, max_val = param_spec\n",
    "            if isinstance(min_val, int): \n",
    "                step = max(1, int(self.bw_factor * (max_val - min_val)))\n",
    "                change = random.choice([-step, step, 0]) \n",
    "                new_val = current_value + change\n",
    "                return np.clip(new_val, min_val, max_val)\n",
    "            else: \n",
    "                std_dev = self.bw_factor * (max_val - min_val)\n",
    "                a, b = (min_val - current_value) / std_dev, (max_val - current_value) / std_dev\n",
    "                new_val = truncnorm.rvs(a, b, loc=current_value, scale=std_dev)\n",
    "                return np.clip(new_val, min_val, max_val)\n",
    "        else:\n",
    "            return current_value\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        print(\"Initializing Harmony Memory...\")\n",
    "        for i in range(self.hms):\n",
    "            while True: \n",
    "                config = self.initialize_harmony()\n",
    "                fitness = self.evaluate_harmony(config)\n",
    "                if fitness > 0.0 or i > self.hms * 2 : # Try a few times to get a valid one, then accept 0\n",
    "                    break\n",
    "                print(\"Initial config invalid, retrying...\")\n",
    "            self.harmony_memory.append({'config': config, 'fitness': fitness})\n",
    "            print(f\"Initial Harmony {i+1}/{self.hms}, Fitness: {fitness:.4f}\")\n",
    "\n",
    "        self.harmony_memory.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        print(f\"\\nInitial Best Fitness: {self.harmony_memory[0]['fitness']:.4f}\")\n",
    "\n",
    "        for impro_count in range(self.max_improvs):\n",
    "            print(f\"\\nImprovisation {impro_count + 1}/{self.max_improvs}\")\n",
    "            \n",
    "            new_harmony_config = {} \n",
    "\n",
    "            structural_keys = ['num_conv_layers', 'num_dense_layers']\n",
    "            temp_structural_values = {}\n",
    "\n",
    "            for key in structural_keys:\n",
    "                if random.random() < self.hmcr: \n",
    "                    parent_idx = random.randrange(self.hms)\n",
    "                    value = self.harmony_memory[parent_idx]['config'][key]\n",
    "                    if random.random() < self.par:\n",
    "                        value = self._pitch_adjust_value(value, key)\n",
    "                else: \n",
    "                    value = self._generate_random_value(key)\n",
    "                temp_structural_values[key] = value\n",
    "\n",
    "            new_harmony_config['num_conv_layers'] = temp_structural_values['num_conv_layers']\n",
    "            new_harmony_config['num_dense_layers'] = temp_structural_values['num_dense_layers']\n",
    " \n",
    "            scalar_params = ['num_epochs', 'optimizer', 'learning_rate', 'batch_size']\n",
    "            for key in scalar_params:\n",
    "                 if random.random() < self.hmcr:\n",
    "                    parent_idx = random.randrange(self.hms)\n",
    "                    value = self.harmony_memory[parent_idx]['config'][key]\n",
    "                    if random.random() < self.par:\n",
    "                        value = self._pitch_adjust_value(value, key)\n",
    "                 else:\n",
    "                    value = self._generate_random_value(key)\n",
    "                 new_harmony_config[key] = value\n",
    "\n",
    "            conv_list_params = ['filters', 'filter_sizes', 'conv_activations', 'pooling_types', 'pool_sizes']\n",
    "            num_cv_layers = new_harmony_config['num_conv_layers']\n",
    "            for key in conv_list_params:\n",
    "                new_harmony_config[key] = []\n",
    "                for _ in range(num_cv_layers):\n",
    "                    if random.random() < self.hmcr:\n",
    "                        parent_idx = random.randrange(self.hms)\n",
    "                        parent_list = self.harmony_memory[parent_idx]['config'][key]\n",
    "                        if len(new_harmony_config[key]) < len(parent_list): \n",
    "                            value = parent_list[len(new_harmony_config[key])]\n",
    "                        else: \n",
    "                            value = random.choice(parent_list) if parent_list else self._generate_random_value(key)\n",
    "                        \n",
    "                        if random.random() < self.par:\n",
    "                             value = self._pitch_adjust_value(value, key, self.harmony_memory[parent_idx]['config'])\n",
    "                    else:\n",
    "                        value = self._generate_random_value(key)\n",
    "                    new_harmony_config[key].append(value)\n",
    "\n",
    "            dense_list_params = ['dense_units', 'dense_activations']\n",
    "            num_ds_layers = new_harmony_config['num_dense_layers']\n",
    "            for key in dense_list_params:\n",
    "                new_harmony_config[key] = []\n",
    "                for _ in range(num_ds_layers):\n",
    "                    if random.random() < self.hmcr:\n",
    "                        parent_idx = random.randrange(self.hms)\n",
    "                        parent_list = self.harmony_memory[parent_idx]['config'][key]\n",
    "                        if len(new_harmony_config[key]) < len(parent_list):\n",
    "                            value = parent_list[len(new_harmony_config[key])]\n",
    "                        else:\n",
    "                            value = random.choice(parent_list) if parent_list else self._generate_random_value(key)\n",
    "\n",
    "                        if random.random() < self.par:\n",
    "                             value = self._pitch_adjust_value(value, key, self.harmony_memory[parent_idx]['config'])\n",
    "                    else:\n",
    "                        value = self._generate_random_value(key)\n",
    "                    new_harmony_config[key].append(value)\n",
    "\n",
    "            new_fitness = self.evaluate_harmony(new_harmony_config)\n",
    "            print(f\"New Harmony improvised. Fitness: {new_fitness:.4f}\")\n",
    "\n",
    "            worst_fitness_in_hm = self.harmony_memory[-1]['fitness']\n",
    "            if new_fitness > worst_fitness_in_hm:\n",
    "                print(f\"New harmony is better than worst in HM ({new_fitness:.4f} > {worst_fitness_in_hm:.4f}). Replacing.\")\n",
    "                self.harmony_memory[-1] = {'config': new_harmony_config, 'fitness': new_fitness}\n",
    "                self.harmony_memory.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "            else:\n",
    "                print(f\"New harmony not better than worst in HM ({new_fitness:.4f} <= {worst_fitness_in_hm:.4f}). Discarding.\")\n",
    "            \n",
    "            print(f\" Current Best Fitness in HM: {self.harmony_memory[0]['fitness']:.4f}\")\n",
    "\n",
    "        best_solution = self.harmony_memory[0]\n",
    "        return best_solution['config'], best_solution['fitness']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53721a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Harmony Memory...\n",
      "Initial Harmony 1/3, Fitness: 0.8521\n",
      "Initial Harmony 2/3, Fitness: 0.9763\n",
      "Initial Harmony 3/3, Fitness: 0.1000\n",
      "\n",
      "Initial Best Fitness: 0.9763\n",
      "\n",
      "Improvisation 1/2\n",
      "New Harmony improvised. Fitness: 0.8031\n",
      "New harmony is better than worst in HM (0.8031 > 0.1000). Replacing.\n",
      " Current Best Fitness in HM: 0.9763\n",
      "\n",
      "Improvisation 2/2\n",
      "New Harmony improvised. Fitness: 0.9049\n",
      "New harmony is better than worst in HM (0.9049 > 0.8031). Replacing.\n",
      " Current Best Fitness in HM: 0.9763\n",
      "num_epochs: 7\n",
      "batch_size: 256\n",
      "num_conv_layers: 1\n",
      "filters: [42]\n",
      "filter_sizes: [3]\n",
      "conv_activations: [4]\n",
      "pooling_types: [1]\n",
      "pool_sizes: [3]\n",
      "num_dense_layers: 2\n",
      "dense_units: [93, 115]\n",
      "dense_activations: [2, 2]\n",
      "optimizer: 2\n",
      "learning_rate: 0.0001\n",
      "Best accuracy: 0.976325\n"
     ]
    }
   ],
   "source": [
    "hsa = HSA(hms=3,hmcr=0.9,par=0.3, bw_factor=0.1,max_improvs=2, dataset_name='digits')\n",
    "bestConfig, bestAcc = hsa.run() \n",
    "for key, value in bestConfig.items(): \n",
    "    print(f\"{key}: {value}\") \n",
    "print(f\"Best accuracy: {bestAcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5479b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Harmony Memory...\n",
      "Initial Harmony 1/10, Fitness: 0.1000\n",
      "Initial Harmony 2/10, Fitness: 0.8253\n",
      "Initial Harmony 3/10, Fitness: 0.9741\n",
      "Initial Harmony 4/10, Fitness: 0.9931\n",
      "Initial Harmony 5/10, Fitness: 0.5711\n",
      "Initial Harmony 6/10, Fitness: 0.9860\n",
      "Initial Harmony 7/10, Fitness: 0.4269\n",
      "Initial Harmony 8/10, Fitness: 0.9231\n",
      "Initial Harmony 9/10, Fitness: 0.9928\n",
      "Initial Harmony 10/10, Fitness: 0.1000\n",
      "\n",
      "Initial Best Fitness: 0.9931\n",
      "\n",
      "Improvisation 1/3\n",
      "New Harmony improvised. Fitness: 0.9910\n",
      "New harmony is better than worst in HM (0.9910 > 0.1000). Replacing.\n",
      " Current Best Fitness in HM: 0.9931\n",
      "\n",
      "Improvisation 2/3\n",
      "New Harmony improvised. Fitness: 0.8050\n",
      "New harmony is better than worst in HM (0.8050 > 0.1000). Replacing.\n",
      " Current Best Fitness in HM: 0.9931\n",
      "\n",
      "Improvisation 3/3\n",
      "New Harmony improvised. Fitness: 0.9273\n",
      "New harmony is better than worst in HM (0.9273 > 0.4269). Replacing.\n",
      " Current Best Fitness in HM: 0.9931\n",
      "num_epochs: 11\n",
      "batch_size: 64\n",
      "num_conv_layers: 3\n",
      "filters: [54, 37, 20]\n",
      "filter_sizes: [3, 5, 3]\n",
      "conv_activations: [3, 4, 2]\n",
      "pooling_types: [2, 1, 0]\n",
      "pool_sizes: [2, 3, 3]\n",
      "num_dense_layers: 2\n",
      "dense_units: [109, 73]\n",
      "dense_activations: [4, 3]\n",
      "optimizer: 1\n",
      "learning_rate: 0.1\n",
      "Best accuracy: 0.993075\n"
     ]
    }
   ],
   "source": [
    "hsa = HSA(hms=10,hmcr=0.9,par=0.3, bw_factor=0.1,max_improvs=3, dataset_name='digits')\n",
    "bestConfig, bestAcc = hsa.run() \n",
    "for key, value in bestConfig.items(): \n",
    "    print(f\"{key}: {value}\") \n",
    "print(f\"Best accuracy: {bestAcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafc9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsa = HSA(hms=20,hmcr=0.9,par=0.3, bw_factor=0.1,max_improvs=2, dataset_name='digits')\n",
    "bestConfig, bestAcc = hsa.run() \n",
    "for key, value in bestConfig.items(): \n",
    "    print(f\"{key}: {value}\") \n",
    "print(f\"Best accuracy: {bestAcc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
