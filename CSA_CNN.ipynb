{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e62926c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, config, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        for i in range(config['num_conv_layers']):\n",
    "            out_channels = config['filters'][i]\n",
    "            kernel_size = config['filter_sizes'][i]\n",
    "            padding = kernel_size // 2  \n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "            self.layers.append(conv)\n",
    "            if config['conv_activations'][i] == 1:  # relu\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif config['conv_activations'][i] == 2:  # tanh \n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif config['conv_activations'][i] == 3:  # linear \n",
    "                self.layers.append(nn.Identity())\n",
    "            elif config['conv_activations'][i] == 4:  # sigmoid\n",
    "                self.layers.append(nn.Sigmoid())\n",
    "            # add pooling if specified aur agar dimension 0 na ho gaya output ki\n",
    "            if config['pooling'][i]:\n",
    "                pool_size = min(config['pool_sizes'][i], 3) # limit pooling size \n",
    "                self.layers.append(nn.MaxPool2d(pool_size))\n",
    "            in_channels = out_channels\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        # calculate flattened size \n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, 28, 28)\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "                if len(x.shape) > 2 and (x.shape[-1] == 0 or x.shape[-2] == 0):\n",
    "                    raise ValueError(\"Invalid architecture - feature maps too small\")\n",
    "            flattened_size = x.shape[1]\n",
    "\t\t\t\t# build dense layers \n",
    "        for i in range(config['num_dense_layers']):\n",
    "            out_features = config['dense_units'][i]\n",
    "            dense = nn.Linear(flattened_size if i == 0 else config['dense_units'][i-1], out_features)\n",
    "            self.layers.append(dense)\n",
    "\n",
    "            # add activation except for the  last layer \n",
    "            if i < config['num_dense_layers'] - 1:\n",
    "                if config['dense_activations'][i] == 1:  \n",
    "                    self.layers.append(nn.ReLU())\n",
    "                elif config['dense_activations'][i] == 2:  \n",
    "                    self.layers.append(nn.Tanh())\n",
    "                elif config['dense_activations'][i] == 3:  \n",
    "                    self.layers.append(nn.Identity())\n",
    "                elif config['dense_activations'][i] == 4: \n",
    "                    self.layers.append(nn.Sigmoid())\n",
    "\n",
    "            flattened_size = out_features\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Linear(flattened_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066b72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSA:\n",
    "    def __init__(self, population_size=10, clone_factor=0.3, mutation_rate=0.1, num_generations=5, dataset_name='digits'):\n",
    "        self.population_size = population_size\n",
    "        self.clone_factor = clone_factor\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.num_generations = num_generations\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        self.train_dataset = torchvision.datasets.EMNIST(root='./data', split=dataset_name,train=True, transform=transform, download=True)\n",
    "        self.test_dataset = torchvision.datasets.EMNIST(root='./data', split=dataset_name,train=False, transform=transform)\n",
    "        self.num_classes = len(self.train_dataset.classes)\n",
    "\n",
    "        self.hyperparameter_ranges = {\n",
    "            'num_epochs': (1, 49),  \n",
    "            'batch_size': (1, 255),\n",
    "            'num_conv_layers': (1, 9), \n",
    "            'filters': (1, 65),\n",
    "            'filter_sizes': (1,9),  \n",
    "            'conv_activations': (1, 4),  # 1: ReLU, 2: Tanh, 3: Linear, 4: Sigmoid\n",
    "            'pooling': (1, 2),  # 1: False, 2: True\n",
    "            'pool_sizes': (2, 9),\n",
    "            'num_dense_layers': (1, 9),  # Reduced for simpler architectures\n",
    "            'dense_units': (1, 65),\n",
    "            'dense_activations': (1, 4), \n",
    "            'optimizer': (1, 5),  # 1: SGD, 2: Adadelta, 3: RMSprop, 4: Adam, 5: Nadam\n",
    "            'learning_rate': [0.0001, 0.001, 0.01]\n",
    "        }\n",
    "\n",
    "    def initialize_antibody(self):\n",
    "        config = {}\n",
    "        config['num_epochs'] = np.random.randint(*self.hyperparameter_ranges['num_epochs'])\n",
    "        config['batch_size'] = np.random.randint(*self.hyperparameter_ranges['batch_size'])\n",
    "        config['num_conv_layers'] = np.random.randint(*self.hyperparameter_ranges['num_conv_layers'])\n",
    "        config['filters'] = [np.random.randint(*self.hyperparameter_ranges['filters']) for _ in range(config['num_conv_layers'])]\n",
    "        config['filter_sizes'] = [np.random.choice([3, 5]) for _ in range(config['num_conv_layers'])] # 3 to 5 co time boht lay raha tha \n",
    "        config['conv_activations'] = [np.random.randint(*self.hyperparameter_ranges['conv_activations']) for _ in range(config['num_conv_layers'])]\n",
    "        config['pooling'] = [np.random.randint(*self.hyperparameter_ranges['pooling']) == 2 for _ in range(config['num_conv_layers'])]\n",
    "        config['pool_sizes'] = [np.random.randint(*self.hyperparameter_ranges['pool_sizes']) for _ in range(config['num_conv_layers'])]\n",
    "        config['num_dense_layers'] = np.random.randint(*self.hyperparameter_ranges['num_dense_layers'])\n",
    "        config['dense_units'] = [np.random.randint(*self.hyperparameter_ranges['dense_units']) for _ in range(config['num_dense_layers'])]\n",
    "        config['dense_activations'] = [np.random.randint(*self.hyperparameter_ranges['dense_activations']) for _ in range(config['num_dense_layers'])]\n",
    "        config['optimizer'] = np.random.randint(*self.hyperparameter_ranges['optimizer'])\n",
    "        config['learning_rate'] = np.random.choice(self.hyperparameter_ranges['learning_rate'])\n",
    "\n",
    "        return config\n",
    "\n",
    "    def evaluate_antibody(self, config): # train cnn return test accuracy \n",
    "        try:\n",
    "            model = CNN(config, self.num_classes).to(device)\n",
    "            if config['optimizer'] == 1:\n",
    "                optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "            elif config['optimizer'] == 2:\n",
    "                optimizer = optim.Adadelta(model.parameters(), lr=config['learning_rate'])\n",
    "            elif config['optimizer'] == 3:\n",
    "                optimizer = optim.RMSprop(model.parameters(), lr=config['learning_rate'])\n",
    "            elif config['optimizer'] == 4:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "            elif config['optimizer'] == 5:\n",
    "                optimizer = optim.NAdam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            train_loader = DataLoader(self.train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "            test_loader = DataLoader(self.test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "            best_accuracy = 0.0\n",
    "            for epoch in range(config['num_epochs']):\n",
    "                model.train()\n",
    "                for images, labels in train_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for images, labels in test_loader:\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "                    accuracy = correct / total\n",
    "                    if accuracy > best_accuracy:\n",
    "                        best_accuracy = accuracy\n",
    "\n",
    "            return best_accuracy\n",
    "        except (ValueError, RuntimeError) as e:\n",
    "            # low accuracy for invalid architectures\n",
    "            print(f\"Invalid architecture encountered: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def mutate_antibody(self, antibody, affinity): # mutation using truchnorm \n",
    "        mutated = antibody.copy()\n",
    "\n",
    "        mutation_prob = np.exp(-self.mutation_rate * (1 - affinity))\n",
    "\n",
    "        for key in mutated:\n",
    "            if key in ['filters', 'filter_sizes', 'conv_activations', 'pooling', 'pool_sizes', 'dense_units', 'dense_activations']:\n",
    "                for i in range(len(mutated[key])):\n",
    "                    if np.random.random() < mutation_prob:\n",
    "                        if key == 'filter_sizes':\n",
    "                            mutated[key][i] = np.random.choice([3, 5])  # Only 3x3 or 5x5 for simplicity\n",
    "                        elif key == 'pooling':\n",
    "                            mutated[key][i] = not mutated[key][i] \n",
    "                        else:\n",
    "                            current = mutated[key][i]\n",
    "                            lb, ub = self.hyperparameter_ranges[key]\n",
    "                            mutated[key][i] = self._truncated_gaussian_mutation(current, lb, ub)\n",
    "            elif key == 'learning_rate':\n",
    "                if np.random.random() < mutation_prob:\n",
    "                    mutated[key] = np.random.choice(self.hyperparameter_ranges[key])\n",
    "            elif key in self.hyperparameter_ranges:\n",
    "                if np.random.random() < mutation_prob:\n",
    "                    current = mutated[key]\n",
    "                    lb, ub = self.hyperparameter_ranges[key]\n",
    "                    mutated[key] = self._truncated_gaussian_mutation(current, lb, ub)\n",
    "        return mutated\n",
    "\n",
    "    def _truncated_gaussian_mutation(self, current, lb, ub, scale=0.1):\n",
    "        mean = current\n",
    "        std = scale * (ub - lb)\n",
    "\n",
    "        a = (lb - mean) / std\n",
    "        b = (ub - mean) / std\n",
    "\n",
    "        new_val = truncnorm.rvs(a, b, loc=mean, scale=std, size=1)[0]\n",
    "\n",
    "        if isinstance(lb, int) and isinstance(ub, int):\n",
    "            new_val = int(round(new_val))\n",
    "\n",
    "        # clip\n",
    "        new_val = max(lb, min(ub, new_val))\n",
    "\n",
    "        return new_val\n",
    "\n",
    "    def run(self):\n",
    "        # initial population \n",
    "        population = [self.initialize_antibody() for _ in range(self.population_size)]\n",
    "        # evaluate population initial \n",
    "        affinities = []\n",
    "        for antibody in population:\n",
    "            accuracy = self.evaluate_antibody(antibody)\n",
    "            affinities.append(accuracy)\n",
    "            print(f\"Initial antibody accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        best_affinity = max(affinities)\n",
    "        best_antibody = population[np.argmax(affinities)]\n",
    "\n",
    "        print(f\"\\nInitial best affinity: {best_affinity:.4f}\")\n",
    "\n",
    "        # evo \n",
    "        for generation in range(self.num_generations):\n",
    "            print(f\"\\nGeneration {generation + 1}/{self.num_generations}\")\n",
    "\t\t\t\t\t\t# selection \n",
    "            selected = []\n",
    "            selected_affinities = []\n",
    "            for antibody, affinity in zip(population, affinities):\n",
    "                if affinity >= best_affinity * 0.9: \n",
    "                    selected.append(antibody)\n",
    "                    selected_affinities.append(affinity)\n",
    "\n",
    "            if not selected:\n",
    "                print(\"No antibodies better than current best, keeping population\")\n",
    "                selected = population.copy()\n",
    "                selected_affinities = affinities.copy()\n",
    "\n",
    "            # create clones proportional to affinity\n",
    "            clones = []\n",
    "            num_clones = max(1,int(self.clone_factor * self.population_size))\n",
    "            if sum(selected_affinities) > 0:\n",
    "              norm_affinities = np.array(selected_affinities) / sum(selected_affinities)\n",
    "            else:\n",
    "              norm_affinities = np.ones(len(selected_affinities)) / len(selected_affinities)\n",
    "\n",
    "            clone_counts = np.random.multinomial(num_clones, norm_affinities)\n",
    "\n",
    "            for antibody, count in zip(selected, clone_counts):\n",
    "                clones.extend([antibody.copy() for _ in range(count)])\n",
    "\n",
    "            # mutation \n",
    "            mutated_clones = []\n",
    "            for clone in clones:\n",
    "                # find clone origin \n",
    "                idx = selected.index(next(ab for ab in selected if ab == clone))\n",
    "                affinity = selected_affinities[idx]\n",
    "\n",
    "                mutated_clone = self.mutate_antibody(clone, affinity)\n",
    "                mutated_clones.append(mutated_clone)\n",
    "\n",
    "            # eval mutated classes \n",
    "            clone_affinities = []\n",
    "            valid_clones = []\n",
    "            for clone in mutated_clones:\n",
    "                accuracy = self.evaluate_antibody(clone)\n",
    "                if accuracy > 0:\n",
    "                  clone_affinities.append(accuracy)\n",
    "                  valid_clones.append(clone)\n",
    "                  print(f\"Clone accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            if not clone:\n",
    "              print(\"All clones were invalid, keeping previous best\")\n",
    "              best_clone_affinity = best_affinity\n",
    "              best_clone = best_antibody\n",
    "            else:\n",
    "              best_clone_idx = np.argmax(clone_affinities)\n",
    "              best_clone_affinity = clone_affinities[best_clone_idx]\n",
    "              best_clone = mutated_clones[best_clone_idx]\n",
    "\n",
    "            # replace if clone is better \n",
    "            if best_clone_affinity > best_affinity:\n",
    "                best_affinity = best_clone_affinity\n",
    "                best_antibody = best_clone\n",
    "                print(f\"New best affinity: {best_affinity:.4f}\")\n",
    "\n",
    "            # new population: best clone + random antibodies\n",
    "            new_population = [best_clone]\n",
    "            new_population.extend([self.initialize_antibody() for _ in range(self.population_size - 1)])\n",
    "\n",
    "            # eval new population \n",
    "            population = new_population\n",
    "            new_affinities = [best_clone_affinity]\n",
    "            for ab in new_population[1:]:\n",
    "                accuracy = self.evaluate_antibody(ab)\n",
    "                new_affinities.append(accuracy)\n",
    "                print(f\"New antibody accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            affinities = new_affinities\n",
    "\n",
    "            avg_affinity = np.mean(affinities)\n",
    "            print(f\"Average affinity: {avg_affinity:.4f}, Best affinity: {best_affinity:.4f}\")\n",
    "\n",
    "        return best_antibody, best_affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad471bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1d62c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 562M/562M [00:31<00:00, 17.9MB/s]\n",
      "Initial antibody accuracy: 0.1000\n",
      "Initial antibody accuracy: 0.9919\n",
      "Initial antibody accuracy: 0.1000\n",
      "Initial antibody accuracy: 0.1013\n",
      "Initial antibody accuracy: 0.1000\n",
      "Initial antibody accuracy: 0.9940\n",
      "Initial antibody accuracy: 0.9261\n",
      "Initial antibody accuracy: 0.9854\n",
      "Initial antibody accuracy: 0.9554\n",
      "Initial antibody accuracy: 0.1000\n",
      "\n",
      "Initial best affinity: 0.9940\n",
      "\n",
      "Generation 1/3\n",
      "Clone accuracy: 0.9889\n",
      "Clone accuracy: 0.9917\n",
      "Clone accuracy: 0.9323\n",
      "New antibody accuracy: 0.9871\n",
      "New antibody accuracy: 0.9906\n",
      "New antibody accuracy: 0.9906\n",
      "New antibody accuracy: 0.5907\n",
      "New antibody accuracy: 0.9681\n",
      "New antibody accuracy: 0.1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csa = CSA(population_size=10, clone_factor=0.3, mutation_rate=0.1, num_generations=3, dataset_name='digits') \n",
    "bestconfig, bacc =csa.run() \n",
    "print('Best configuration found: ') \n",
    "for key, value in bestconfig.items(): \n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best Accuracy : {bacc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a11f46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial antibody accuracy: 0.9768\n",
      "Initial antibody accuracy: 0.9868\n",
      "\n",
      "Initial best affinity: 0.9868\n",
      "\n",
      "Generation 1/3\n",
      "Clone accuracy: 0.9912\n",
      "New best affinity: 0.9912\n",
      "New antibody accuracy: 0.9765\n",
      "Average affinity: 0.9839, Best affinity: 0.9912\n",
      "\n",
      "Generation 2/3\n",
      "Clone accuracy: 0.9932\n",
      "New best affinity: 0.9932\n",
      "New antibody accuracy: 0.9727\n",
      "Average affinity: 0.9829, Best affinity: 0.9932\n",
      "\n",
      "Generation 3/3\n",
      "Clone accuracy: 0.9407\n",
      "New antibody accuracy: 0.1000\n",
      "Average affinity: 0.5204, Best affinity: 0.9932\n",
      "\n",
      "Best configuration found:\n",
      "num_epochs: 9\n",
      "batch_size: 170\n",
      "num_conv_layers: 2\n",
      "filters: [26, 47]\n",
      "filter_sizes: [3, 5]\n",
      "conv_activations: [3, 3]\n",
      "pooling: [False, False]\n",
      "pool_sizes: [3, 3]\n",
      "num_dense_layers: 2\n",
      "dense_units: [44, 112]\n",
      "dense_activations: [3, 2]\n",
      "optimizer: 3\n",
      "learning_rate: 0.001\n",
      "\n",
      "Best accuracy: 0.9932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csa = CSA(population_size=2, clone_factor=0.3, mutation_rate=0.1, num_generations=3, dataset_name='digits') \n",
    "bestconfig, bacc =csa.run() \n",
    "print('Best configuration found: ') \n",
    "for key, value in bestconfig.items(): \n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best Accuracy : {bacc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Initial antibody accuracy: 0.1000\n",
      "Initial antibody accuracy: 0.9926\n",
      "Initial antibody accuracy: 0.9937\n",
      "\n",
      "Initial best affinity: 0.9937\n",
      "\n",
      "Generation 1/2\n",
      "Clone accuracy: 0.9926\n",
      "New antibody accuracy: 0.9798\n",
      "New antibody accuracy: 0.9889\n",
      "Average affinity: 0.9871, Best affinity: 0.9937\n",
      "\n",
      "Generation 2/2\n",
      "Clone accuracy: 0.8450\n",
      "New antibody accuracy: 0.9703\n",
      "New antibody accuracy: 0.9463\n",
      "Average affinity: 0.9205, Best affinity: 0.9937\n",
      "\n",
      "Best configuration found:\n",
      "num_epochs: 6\n",
      "batch_size: 84\n",
      "num_conv_layers: 4\n",
      "filters: [29, 53, 56, 60]\n",
      "filter_sizes: [5, 3, 3, 3]\n",
      "conv_activations: [1, 2, 1, 3]\n",
      "pooling: [True, True, False, False]\n",
      "pool_sizes: [3, 2, 3, 2]\n",
      "num_dense_layers: 1\n",
      "dense_units: [48]\n",
      "dense_activations: [3]\n",
      "optimizer: 4\n",
      "learning_rate: 0.0001\n",
      "\n",
      "Best accuracy: 0.9937\n",
      "      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "csa = CSA(population_size=3, clone_factor=0.3, mutation_rate=0.1, num_generations=2, dataset_name='digits') \n",
    "bestconfig, bacc =csa.run() \n",
    "print('Best configuration found: ') \n",
    "for key, value in bestconfig.items(): \n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Best Accuracy : {bacc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
